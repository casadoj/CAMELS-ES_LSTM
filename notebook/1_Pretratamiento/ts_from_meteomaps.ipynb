{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b77dc0-4a95-4bfe-9751-137c57c26fa3",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "This is a script to calculate accumulated precipitation time series by calibration catchment.\n",
    "* Precip is accumulated in time and on the catchments area.\n",
    "* Output is in [precip unit]/[area unit] i.e. mm/km2\n",
    "* Accumulation period is selected with a flag.\n",
    "\n",
    "The scrip can also do rolling accumulation over the entire period.\n",
    "\n",
    "Note: hard coded variables names \"Band1\" \"pixarea\" \"pr6\"\n",
    "\n",
    "Usage:\n",
    "\n",
    "```ts_from_meteomaps.py  -i /path_to/pr6.nc -I /path_to/catchments.nc -O /path_to/output.csv -t sum_month -s 201701011200 -e 201712311200 -A /path_to/pixarea.nc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8bdcda-7b4e-426d-94a2-eefca11b8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e76012-3ff3-447b-a5c0-cdb25779461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarg():\n",
    "    \"\"\" Get program arguments.\n",
    "\n",
    "    :return: args:  namespace of program arguments\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', '--datadir', type=str, required=True,\n",
    "                        help='data folder')\n",
    "    parser.add_argument('-I', '--interbasins', type=str, required=True,\n",
    "                        help='Interbasins file')\n",
    "    parser.add_argument('-O', '--outputfilename', type=str, required=False, default=\"indexes.csv\",\n",
    "                        help='Output file')\n",
    "    parser.add_argument('-s', '--startdate', type=str, required=True,\n",
    "                        help='Start date for evaluation [YYYYMMDDhhmm]')\n",
    "    parser.add_argument('-e', '--enddate', type=str, required=True,\n",
    "                        help='End date for evaluation [YYYYMMDDhhmm]')\n",
    "    parser.add_argument('-t', '--ctype', type=str, required=False, default='All',\n",
    "                        help='Aggregation type [sum_all|sum_year|ts]')\n",
    "    parser.add_argument('-A', '--interbasins_area', type=str, required=True,\n",
    "                        help='Interbasins area file')\n",
    "    args = parser.parse_args()  # assign namespace to args\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7056a122-dc00-496d-93b9-f52915cc6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series(pr6, ibasins, area):\n",
    "    \"\"\" Calc time series of accumulated values for catchment areas (ibasins) from DataArray (pr).\n",
    "\n",
    "        Put the time series in a DataFrame using time as index and catchment code as header.\n",
    "\n",
    "    :return: tssdata: pandas.DataFrame with time series of accumulated values by catchment\n",
    "    \"\"\"\n",
    "    # Calc accumulated values over catchments\n",
    "    area_tot_group = area.groupby(ibasins).sum()    #[m2]\n",
    "    # Put in dataframe\n",
    "    area_tot_df = area_tot_group.to_dataframe()     #[m2]\n",
    "\n",
    "    # calc volume for each cell\n",
    "    pr = pr6 * area  #[m3]\n",
    "    # group cells in pr DataArray by catchment ID (ibasins)\n",
    "    pr6_ts_group = pr.groupby(ibasins)  #[m3]\n",
    "    # sum over catchments ID\n",
    "    #pr6_ts = pr6_ts_group.sum(dim='stacked_y_x')    #[m3]\n",
    "    pr6_ts = pr6_ts_group.sum(dim='stacked_lat_lon')    #[m3]\n",
    "    # put in multi-index dataframe\n",
    "    pr6_ts_df = pr6_ts.to_dataframe(name='pr6')       #[m3]\n",
    "    # divide by catchments area\n",
    "    #pr6_ts_df['pr6'] = pr6_ts_df['pr6'].div(area_tot_df['pixarea'].values,axis=0)   #[m]\n",
    "\n",
    "    # generate single index dataframe and store time series\n",
    "    dates = pr6_ts_df.index.unique(level=\"time\").values\n",
    "    catcodes = pr6_ts_df.index.unique(level=\"ibasins\").values\n",
    "    tssdata = pd.DataFrame(index = dates)\n",
    "    for code in catcodes:\n",
    "        # print(code)\n",
    "        # extract ts for one catchment from multi-*index dataframe\n",
    "        ts = pr6_ts_df.xs(code, level='ibasins', drop_level=True)     #[m3]\n",
    "        # average by catchment area\n",
    "        ts_avg = pd.DataFrame()\n",
    "        ts_avg['pr6'] = ts['pr6'] / area_tot_df.loc[code,'pixarea'] * 1000.     #[mm]\n",
    "        # use code as header\n",
    "        ts_avg = ts_avg.rename(columns={'pr6': str(int(code))})\n",
    "        # store in dataframe\n",
    "        tssdata = pd.merge(tssdata,ts_avg,how='outer',left_index=True,right_index=True, copy=False)\n",
    "    return tssdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939317ac-319b-4139-9c4a-3bb5a82c883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_total_accumulation(pr6, ibasins, area):\n",
    "    \"\"\" Calc accumulated values for catchment areas (ibasins) from DataArray (pr) over entire time period.\n",
    "\n",
    "        Put accumulated values in a DataFrame with ObsID as index.\n",
    "\n",
    "    :return: tssdata: pandas.DataFrame with accumulated values by catchment\n",
    "    \"\"\"\n",
    "    # Calc accumulated precip volume over time\n",
    "    pr6_sum = pr6.sum(dim='time')   #[m]\n",
    "    pr6_tot = pr6_sum * area  #[m3]\n",
    "\n",
    "    # Calc accumulated values over catchments\n",
    "    pr6_tot_group = pr6_tot.groupby(ibasins).sum()  #[m3]\n",
    "    area_tot_group = area.groupby(ibasins).sum()    #[m2]\n",
    "\n",
    "    # Put in dataframe\n",
    "    pr6_tot_df = pr6_tot_group.to_dataframe(name='pr6')\n",
    "    area_tot_df = area_tot_group.to_dataframe()\n",
    "\n",
    "    # Divide precip volume [m3] by catchment area [m2] and transform to [mm]\n",
    "    pr6_tot_df['pr6'] = pr6_tot_df['pr6'].div(area_tot_df['pixarea'].values,axis=0) * 1000.  #[mm]\n",
    "\n",
    "    # rename index\n",
    "    pr6_tot_df.index.name = 'ObsID'\n",
    "    # set index to integer\n",
    "    pr6_tot_df.index = pr6_tot_df.index.astype('int', copy=False)\n",
    "    # rename column\n",
    "    pr6_tot_df.rename(columns={'pr6': 'pr6_tot'})\n",
    "    return pr6_tot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e6015c-10ee-4575-9f46-c411d9bac628",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid or missing encoding declaration for 'ts_from_meteomaps.py' (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  File \u001b[1;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\tokenize.py:330\u001b[1;36m in \u001b[1;35mfind_cookie\u001b[1;36m\n\u001b[1;33m    line_string = line.decode('utf-8')\u001b[1;36m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m\u001b[1;31m:\u001b[0m 'utf-8' codec can't decode byte 0xb8 in position 10: invalid start byte\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3433\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn [2], line 1\u001b[0m\n    get_ipython().run_line_magic('load', 'ts_from_meteomaps')\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2364\u001b[0m in \u001b[0;35mrun_line_magic\u001b[0m\n    result = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\site-packages\\IPython\\core\\magics\\code.py:359\u001b[0m in \u001b[0;35mload\u001b[0m\n    contents = self.shell.find_user_code(args, search_ns=search_ns)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3741\u001b[0m in \u001b[0;35mfind_user_code\u001b[0m\n    return openpy.read_py_file(tgt, skip_encoding_cookie=skip_encoding_cookie)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\site-packages\\IPython\\utils\\openpy.py:77\u001b[0m in \u001b[0;35mread_py_file\u001b[0m\n    with open(filepath) as f:  # the open function defined in this module.\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\tokenize.py:394\u001b[0m in \u001b[0;35mopen\u001b[0m\n    encoding, lines = detect_encoding(buffer.readline)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\tokenize.py:371\u001b[0m in \u001b[0;35mdetect_encoding\u001b[0m\n    encoding = find_cookie(first)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mC:\\ProgramFiles\\Anaconda3\\envs\\xr\\lib\\tokenize.py:335\u001b[1;36m in \u001b[1;35mfind_cookie\u001b[1;36m\n\u001b[1;33m    raise SyntaxError(msg)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid or missing encoding declaration for 'ts_from_meteomaps.py'\n"
     ]
    }
   ],
   "source": [
    "#-- settings\n",
    "# datadir = '/perm/mo/mocm/proj/efas/hprot/GRIDS_5x5km_efas4_dataset_new_sheremap/'\n",
    "# interbasinsfile = '/perm/mo/mocm/proj/efas/hres/meteo_test/pcraster_scripts/efas4_cats.nc'\n",
    "# outputfile = '/perm/mo/mocm/proj/efas/hres/meteo_test/ef4_dataset_new_sheremap_ts.csv'\n",
    "\n",
    "def main():\n",
    "    # read settings\n",
    "    args = getarg()\n",
    "    datadir = args.datadir\n",
    "    interbasinsfile= args.interbasins\n",
    "    areafile = args.interbasins_area\n",
    "    outputfile = args.outputfilename\n",
    "    startdate = datetime.datetime.strptime(args.startdate, '%Y%m%d%H%M')\n",
    "    enddate = datetime.datetime.strptime(args.enddate, '%Y%m%d%H%M')\n",
    "    flag_type = args.ctype\n",
    "\n",
    "    ### Read inputs\n",
    "    # Get precip\n",
    "    extension = os.path.splitext(datadir)[1]\n",
    "    if extension == '.nc':\n",
    "        # get from one single .nc file\n",
    "        fdata = datadir\n",
    "        DS = xr.open_dataset(fdata, chunks={'time': 'auto'})\n",
    "    else:\n",
    "        # Get forcings from multiple nc files\n",
    "        mfdataDIR = datadir + 'pr6_*'\n",
    "        DS = xr.open_mfdataset(mfdataDIR, chunks={'time': 'auto'})\n",
    "    for name in list(DS.data_vars):\n",
    "        if name == 'precip':\n",
    "            DS = DS.rename({\"precip\": \"pr6\"})\n",
    "\n",
    "    # Get cell area map\n",
    "    DS_area = xr.open_dataset(areafile)\n",
    "    # Sort Lat (just in case)\n",
    "    DS_area = DS_area.sortby('lat', ascending=False)\n",
    "    for name in list(DS_area.data_vars):\n",
    "        if name == 'Band1':\n",
    "            DS_area = DS_area.rename({\"Band1\": \"pixarea\"})\n",
    "    area = DS_area.pixarea  #[m2]\n",
    "    #area = area / area  # this is to basically remove area (:D)\n",
    "    # put in numpy array\n",
    "    areanp = area.values\n",
    "    # assign to dataset\n",
    "    DS[\"pixarea\"] = (['lat','lon'],  areanp)\n",
    "\n",
    "    # Get catchments masks from nc file\n",
    "    DS_ibasins = xr.open_dataset(interbasinsfile)\n",
    "    # Flip Lat coord because map comes from pcraster\n",
    "    DS_ibasins = DS_ibasins.sortby('lat', ascending=False)\n",
    "    ibasins = DS_ibasins.Band1\n",
    "    ibasins = ibasins.where(ibasins != 0)\n",
    "    # put it in numpy array\n",
    "    ibasinsnp = ibasins.values\n",
    "    # assign to dataset\n",
    "    #DS[\"ibasins\"]=(['y','x'],  ibasinsnp)\n",
    "    DS[\"ibasins\"]=(['lat','lon'],  ibasinsnp)\n",
    "\n",
    "    # Extract variable pr6 from array.Dataset for the selected period and put in xarray.DataArray\n",
    "    pr6 = DS.pr6.sel(time=slice(startdate, enddate)) * 0.25 / 1000.     #input data in mm/day and Dt=6hrs -> mm -> m\n",
    "    pixarea = DS.pixarea\n",
    "    ibas = DS.ibasins\n",
    "\n",
    "    # # Calc accumulated values over catchments\n",
    "    # area_tot_group = pixarea.groupby(ibas).sum()    #[m2]\n",
    "    # # Put in dataframe\n",
    "    # area_tot_df = area_tot_group.to_dataframe() / 1000000.  #[km2]\n",
    "\n",
    "    # Calc accumulated values by catchment with sum pr6 values over full period\n",
    "    if flag_type == 'sum_tot' :\n",
    "        tssdata = extract_total_accumulation(pr6,ibas,pixarea)\n",
    "        print(flag_type)\n",
    "\n",
    "    # Calc accumulated values by catchment with sum pr6 values over years\n",
    "    # Calc yearly time series accumulated by catchment\n",
    "    if flag_type == 'sum_year':\n",
    "        # accumulate by year\n",
    "        pr = pr6.resample(time=\"Y\").sum()\n",
    "        tssdata = extract_time_series(pr,ibas,pixarea)\n",
    "        print(flag_type)\n",
    "\n",
    "    if flag_type == 'sumcum_year':\n",
    "        # accumulate by year\n",
    "        pr = pr6.resample(time=\"Y\").sum()\n",
    "        tssdata = extract_time_series(pr,ibas,pixarea)\n",
    "        tssdata = tssdata.cumsum(axis='index')\n",
    "        print(flag_type)\n",
    "\n",
    "    # Calc monthly time series accumulated by catchment\n",
    "    if flag_type == 'sum_month':\n",
    "        # accumulate by month\n",
    "        pr = pr6.resample(time=\"M\").sum()\n",
    "        tssdata = extract_time_series(pr,ibas,pixarea)\n",
    "        print(flag_type)\n",
    "\n",
    "    if flag_type == 'sumcum_month':\n",
    "        # accumulate by month\n",
    "        pr = pr6.resample(time=\"M\").sum()\n",
    "        tssdata = extract_time_series(pr,ibas,pixarea)\n",
    "        tssdata = tssdata.cumsum(axis='index')\n",
    "        print(flag_type)\n",
    "\n",
    "    # Calc time series accumulated by catchment\n",
    "    if flag_type == 'ts':\n",
    "        # no accumulation\n",
    "        # pr = pr6\n",
    "        tssdata = extract_time_series(pr6,ibas,pixarea)\n",
    "        print(flag_type)\n",
    "\n",
    "    if flag_type == 'sumcum_ts':\n",
    "        # no accumulation\n",
    "        # pr = pr6\n",
    "        tssdata = extract_time_series(pr6,ibas,pixarea)\n",
    "        tssdata = tssdata.cumsum(axis='index')\n",
    "        print(flag_type)\n",
    "\n",
    "    # print results TO CSV FILE\n",
    "    with open(outputfile, 'w') as outf:\n",
    "        tssdata.to_csv(outf,sep=',')\n",
    "\n",
    "    # # print results TO CSV FILE\n",
    "    # with open('/perm/mo/mocm/proj/efas/hres/meteo_test/interpolation/sph_2018-08-29_ef4_1min/area_bas_efas5.csv', 'w') as outf:\n",
    "    #     area_tot_df.to_csv(outf,sep=',')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e5f84-405a-44e4-a3b2-862a9a7706a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
